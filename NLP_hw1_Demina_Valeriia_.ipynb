{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11043497,"sourceType":"datasetVersion","datasetId":6879271}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Build SVM model with TF-IDF, BoW models ","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Preprocessing train and test tweet data","metadata":{}},{"cell_type":"code","source":"pip install wordninja","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:04:33.693676Z","iopub.execute_input":"2025-03-19T15:04:33.693992Z","iopub.status.idle":"2025-03-19T15:04:39.984467Z","shell.execute_reply.started":"2025-03-19T15:04:33.693963Z","shell.execute_reply":"2025-03-19T15:04:39.983630Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting wordninja\n  Downloading wordninja-2.0.0.tar.gz (541 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: wordninja\n  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541530 sha256=4fce30e8d2f0acb5749338629ffe4c155b7f5b7fd451dd895b4257a425c30f0d\n  Stored in directory: /root/.cache/pip/wheels/aa/44/3a/f2a5c1859b8b541ded969b4cd12d0a58897f12408f4f51e084\nSuccessfully built wordninja\nInstalling collected packages: wordninja\nSuccessfully installed wordninja-2.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re, string, wordninja, pandas as pd, numpy as np, time\nimport string\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:04:53.245434Z","iopub.execute_input":"2025-03-19T15:04:53.245787Z","iopub.status.idle":"2025-03-19T15:04:54.840940Z","shell.execute_reply.started":"2025-03-19T15:04:53.245762Z","shell.execute_reply":"2025-03-19T15:04:54.840237Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"### For tokenisation we will use  NLTK’s TweetTokenizer, because it is better at handling typical tweet elements \n### (mentions, hashtags, emojis, urls) than a simple  whitespace, regex tokenizer pr spacy.\n### It keeps tweet-specific elements intact\n\n\n\n#### For the preprocessing we are goinng to do the following steps:\n\n**We remove Stopwords :**\n\nWe remove standard eng stopwords (like “the,” “and”) but keep words like “not” “no” to keeep negation cues for classification.\n\n**We remove Mentions and timestamps:**\n\nWe replace user mentions with a placeholder “@user,” this preserves the idea of someone being mentioned without leaking specific usernames. \nTimestamps (12/10, 13 pm, 13:34am) get converted to “timestamp” so the model isn’t distracted by exact times.\n \n##### We remove hashtags from #hashtaged words \n\n##### We remove non-ASCII and non-alphanumeric chars except spaces, standalone punctuation \n\n**Splitting “n’t”:** we split :“didn’t” → “did” + “n’t” and \"n't\"-> \"not\" to make negation more explicit\n\n#### We include keyword (train_df['keyword']) and location ((train_df['location']) related to each tweet into a tweet and do the preprocessing step on this new compound tweet. We put keyword an the beginning and at the end of each tweet\n\n## We use Stemming (SnowballStemmer) for tweet preprocessing\n\n We choose stemming over lemmatization to reduce words to their base forms.\nI have done several SVM runs with both smemming with SnowballStemmer , Lemmatization with WordLemmatizer and spaCy and\n\nStemming with SnowballStemmer gives the best outcomes outperforming WordLEmmatizer and spaCy\n\nSnowballStemmer is more efficient probably because its aggressive reduction of words can leads to a smaller, more consolidated feature space. this reduction might help boost frequency counts for key tokens and reduce noise due to inconsistent morphological variants\n ","metadata":{}},{"cell_type":"code","source":"# 1 Compile regex patterns\nurl_pattern = re.compile(r'http\\S+')\ntimestamp_pattern = re.compile(\n    r'\\b\\d{1,2}[/:]\\d{1,2}(?:/\\d{2,4})?\\b'     # e.g. 12:30, 12/08/2025, etc.\n    r'|\\d{1,2}\\s*(?:am|pm)'                  # e.g. 12 am, 3pm\n    r'|\\b\\d{1,2}:\\d{2}(?:am|pm)?\\b',         # e.g. 12:30pm\n    re.IGNORECASE\n)\nsmiley_pattern = re.compile(r'[:;=8][\\-o]?[\\)\\]\\(\\[dDpP]|<3')  # e.g. :) :D etc.\n\n# 2 removing negations from default_stop \n\ndefault_stop = set(stopwords.words('english'))\nnegation_words = {\n    \"not\", \"no\", \"nor\", \"don\", \"don't\", \"ain\", \"aren\", \"aren't\",\n    \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \n    \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\",\n    \"isn\", \"isn't\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\",\n    \"needn\", \"needn't\", \"shan\", \"shan't\", \"shouldn\", \"shouldn't\",\n    \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"won\", \"won't\",\n    \"wouldn\", \"wouldn't\"\n}\ncustom_stop = {w for w in default_stop if w not in negation_words}\n\ntokenizer = TweetTokenizer()\n\nstemmer = SnowballStemmer('english')\n\n\n\ndef preprocess_tweet(text):\n    text = text.lower()\n    \n    # Replace URLs, timestamps, smileys\n    text = url_pattern.sub('url', text)\n    text = timestamp_pattern.sub('timestamp', text)\n    text = smiley_pattern.sub('smiley', text)\n    \n    # Replace @handles with \"@user\"\n    text = re.sub(r'@\\w+', '@user', text)\n    \n    # Remove commas within numbers (13,000 -> 13000)\n    text = re.sub(r'(?<=\\d),(?=\\d)', '', text)\n\n    # Split hashtags: remove '#' and split compound words using wordninja\n    def hashtag_split(match):\n        return \" \" + \" \".join(wordninja.split(match.group(1))) + \" \"\n    text = re.sub(r'#(\\w+)', hashtag_split, text)\n    \n    # Remove non-ASCII\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    # Remove other non-alphanumeric chars except spaces\n    text = re.sub(r'[^a-z0-9\\s]', '', text)\n    \n    # Tokenize\n    tokens = tokenizer.tokenize(text)\n\n    # \"n't\" --> 'not'\n    sep_tokens = []\n    for tok in tokens:\n        if tok.endswith(\"n't\") and len(tok) > 3:\n            main_part = tok[:-3]\n            sep_tokens.append(main_part)\n            sep_tokens.append(\"not\")  \n        else:\n            sep_tokens.append(tok)\n    tokens = sep_tokens\n\n    tokens = [t for t in tokens if t != 'pm']\n    \n    # Remove stopwords\n    tokens = [t for t in tokens if t not in custom_stop]\n    \n    # Remove standalone punctuation\n    tokens = [t for t in tokens if t not in string.punctuation]\n\n    #  Stemming with Snowballstemmer\n    tokens = [stemmer.stem(t) for t in tokens]\n\n    # Rejoin\n    return \" \".join(tokens)\n\ntrain_df = pd.read_csv(\"/kaggle/input/dattaset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/dattaset/test.csv\")\n\n# Include keyword + location in tweet and preprocess tweet for train data\ntrain_df['keyword'] = train_df['keyword'].fillna('')\ntrain_df['location'] = train_df['location'].fillna('')\n\ntrain_df['combined_text'] = train_df['keyword'] + ' ' + train_df['text'] + ' ' + train_df['location'] + ' ' + train_df['keyword']\ntrain_df['processed_text'] = train_df['combined_text'].apply(preprocess_tweet)\nprint(train_df[['text','processed_text']].head())\n\n# Include keyword + location in tweet and preprocess tweet for test data\ntest_df['keyword'] = test_df['keyword'].fillna('')\ntest_df['location'] = test_df['location'].fillna('')\n\ntest_df['combined_text'] = test_df['keyword'] + ' ' + test_df['text'] + ' ' + test_df['location'] + ' '  + test_df['keyword']\ntest_df['processed_text'] = test_df['combined_text'].apply(preprocess_tweet)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:04:54.216011Z","iopub.execute_input":"2025-03-19T16:04:54.216369Z","iopub.status.idle":"2025-03-19T16:04:57.121644Z","shell.execute_reply.started":"2025-03-19T16:04:54.216347Z","shell.execute_reply":"2025-03-19T16:04:57.120957Z"}},"outputs":[{"name":"stdout","text":"                                                text  \\\n0  Our Deeds are the Reason of this #earthquake M...   \n1             Forest fire near La Ronge Sask. Canada   \n2  All residents asked to 'shelter in place' are ...   \n3  13,000 people receive #wildfires evacuation or...   \n4  Just got sent this photo from Ruby #Alaska as ...   \n\n                                      processed_text  \n0          deed reason earthquak may allah forgiv us  \n1               forest fire near la rong sask canada  \n2  resid ask shelter place notifi offic no evacu ...  \n3  13000 peopl receiv wildfir evacu order california  \n4  got sent photo rubi alaska smoke wildfir pour ...  \n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"train_df['processed_text'].to_csv('processed_tweets_stem.txt', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:05:14.588022Z","iopub.execute_input":"2025-03-19T16:05:14.588317Z","iopub.status.idle":"2025-03-19T16:05:14.610607Z","shell.execute_reply.started":"2025-03-19T16:05:14.588297Z","shell.execute_reply":"2025-03-19T16:05:14.609809Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"print(train_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:05:18.178423Z","iopub.execute_input":"2025-03-19T16:05:18.178763Z","iopub.status.idle":"2025-03-19T16:05:18.183426Z","shell.execute_reply.started":"2025-03-19T16:05:18.178738Z","shell.execute_reply":"2025-03-19T16:05:18.182427Z"}},"outputs":[{"name":"stdout","text":"Index(['id', 'keyword', 'location', 'text', 'target', 'combined_text',\n       'processed_text'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"counts = train_df['target'].value_counts()\nprint(counts)\n\nmax_words = train_df['processed_text'].apply(lambda x: len(x.split())).max()\nprint(\"Longest entry word count:\", max_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:25:31.006588Z","iopub.execute_input":"2025-03-19T16:25:31.006997Z","iopub.status.idle":"2025-03-19T16:25:31.023905Z","shell.execute_reply.started":"2025-03-19T16:25:31.006965Z","shell.execute_reply":"2025-03-19T16:25:31.023193Z"}},"outputs":[{"name":"stdout","text":"target\n0    4342\n1    3271\nName: count, dtype: int64\nLongest entry word count: 31\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"## 1.2. TF-IDF, BoW vectorizers","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:05:31.274598Z","iopub.execute_input":"2025-03-19T16:05:31.274915Z","iopub.status.idle":"2025-03-19T16:05:31.278877Z","shell.execute_reply.started":"2025-03-19T16:05:31.274893Z","shell.execute_reply":"2025-03-19T16:05:31.277910Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":" Tried to perform BoW and TF-IDF with different parameters, like including n-grams up to trigrams and experimenting with vocab limits, min_df, max_df -- nothing significantly contriburing to model performance:","metadata":{}},{"cell_type":"markdown","source":"BoW vectorizer:","metadata":{}},{"cell_type":"code","source":"# initialize BoW vectorizer\nbow_vectorizer = CountVectorizer(encoding='utf8')\n\nbow_vect2 = CountVectorizer(\n encoding='utf8',\n ngram_range=(1,3),\n # min_df=2,\n # max_df=0.95,\n # max_features=10000,\n # binary=False\n)\n\n# fit BoW vectorizer on train_df\nbow_vectorizer.fit(train_df['processed_text'])\nbow_vect2.fit(train_df['processed_text'])\n\nX_train_bow = bow_vectorizer.transform(train_df['processed_text'])\nX_test_bow = bow_vectorizer.transform(test_df['processed_text'])\n\n# X_train_bow2 = bow_vect2.transform(train_df['processed_text'])\n# X_test_bow2 = bow_vect2.transform(test_df['processed_text'])\n\n\nprint(X_train_bow)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:06:43.149046Z","iopub.execute_input":"2025-03-19T16:06:43.149331Z","iopub.status.idle":"2025-03-19T16:06:44.223527Z","shell.execute_reply.started":"2025-03-19T16:06:43.149311Z","shell.execute_reply":"2025-03-19T16:06:44.222701Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"  (0, 1067)\t1\n  (0, 3516)\t1\n  (0, 4046)\t1\n  (0, 4832)\t1\n  (0, 7614)\t1\n  (0, 9829)\t1\n  (0, 12473)\t1\n  (1, 2424)\t1\n  (1, 4695)\t1\n  (1, 4826)\t1\n  (1, 6893)\t1\n  (1, 8267)\t1\n  (1, 10200)\t1\n  (1, 10413)\t1\n  (2, 1396)\t1\n  (2, 4373)\t1\n  (2, 4436)\t1\n  (2, 8400)\t1\n  (2, 8480)\t1\n  (2, 8620)\t1\n  (2, 8755)\t1\n  (2, 9234)\t2\n  (2, 9998)\t1\n  (2, 10704)\t2\n  (3, 119)\t1\n  :\t:\n  (7610, 12467)\t1\n  (7610, 12508)\t1\n  (7610, 12699)\t1\n  (7611, 2465)\t1\n  (7611, 2940)\t1\n  (7611, 4069)\t2\n  (7611, 6244)\t1\n  (7611, 6335)\t1\n  (7611, 7187)\t1\n  (7611, 8427)\t1\n  (7611, 9313)\t1\n  (7611, 9365)\t1\n  (7611, 10085)\t1\n  (7611, 10612)\t1\n  (7611, 11450)\t1\n  (7611, 11884)\t1\n  (7612, 824)\t1\n  (7612, 2389)\t1\n  (7612, 5843)\t1\n  (7612, 6979)\t1\n  (7612, 8327)\t1\n  (7612, 8456)\t1\n  (7612, 9794)\t1\n  (7612, 12467)\t1\n  (7612, 13004)\t1\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"TF-IDF vectorizer:","metadata":{}},{"cell_type":"code","source":"# initialize TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(encoding='utf8')\n\ntfidf_vect = TfidfVectorizer(\n encoding='utf8',\n ngram_range=(1,3),\n # max_df=0.9,\n # min_df=5,\n # sublinear_tf=True\n)\n\ntfidf_vectorizer.fit(train_df['processed_text'])\ntfidf_vect.fit(train_df['processed_text'])\n\nX_train_tfidf = tfidf_vectorizer.transform(train_df['processed_text'])\nX_test_tfidf = tfidf_vectorizer.transform(test_df['processed_text'])\n\nX_train_tfidf2 = tfidf_vect.transform(train_df['processed_text'])\nX_test_tfidf2 = tfidf_vect.transform(test_df['processed_text'])\n\n\nprint(X_train_tfidf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:07:06.825832Z","iopub.execute_input":"2025-03-19T16:07:06.826121Z","iopub.status.idle":"2025-03-19T16:07:07.060590Z","shell.execute_reply.started":"2025-03-19T16:07:06.826101Z","shell.execute_reply":"2025-03-19T16:07:07.059844Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"  (0, 12473)\t0.2565252442235441\n  (0, 9829)\t0.35066653098778766\n  (0, 7614)\t0.29726933026488295\n  (0, 4832)\t0.4655483796694387\n  (0, 4046)\t0.3190347569931321\n  (0, 3516)\t0.4812100428181675\n  (0, 1067)\t0.4156647123744579\n  (1, 10413)\t0.5032480027565118\n  (1, 10200)\t0.5263327936570664\n  (1, 8267)\t0.3135319388658582\n  (1, 6893)\t0.3453931599445707\n  (1, 4826)\t0.32473146237564765\n  (1, 4695)\t0.23343057690600238\n  (1, 2424)\t0.3036052857345438\n  (2, 10704)\t0.5875709533528314\n  (2, 9998)\t0.27192514823013375\n  (2, 9234)\t0.45255761102091774\n  (2, 8755)\t0.22536571706589506\n  (2, 8620)\t0.22032519323177235\n  (2, 8480)\t0.33340700261820133\n  (2, 8400)\t0.1588433388437048\n  (2, 4436)\t0.23018153570499789\n  (2, 4373)\t0.18258108347852875\n  (2, 1396)\t0.2323036222883481\n  (3, 13004)\t0.3288533262988225\n  :\t:\n  (7610, 7396)\t0.48925003246654286\n  (7610, 5616)\t0.4173588017779799\n  (7610, 605)\t0.44293175501353865\n  (7611, 11884)\t0.24822566286151615\n  (7611, 11450)\t0.25059447707181515\n  (7611, 10612)\t0.23507593492548925\n  (7611, 10085)\t0.2698084306614959\n  (7611, 9365)\t0.2657644457891202\n  (7611, 9313)\t0.17381842646163045\n  (7611, 8427)\t0.3034844064882237\n  (7611, 7187)\t0.20488761878765877\n  (7611, 6335)\t0.2000608516859879\n  (7611, 6244)\t0.18709068285550015\n  (7611, 4069)\t0.6069688129764474\n  (7611, 2940)\t0.19162030790159096\n  (7611, 2465)\t0.1801307477050271\n  (7612, 13004)\t0.34791254017647205\n  (7612, 12467)\t0.10503678492080601\n  (7612, 9794)\t0.4005792513836138\n  (7612, 8456)\t0.3582029107146578\n  (7612, 8327)\t0.276853583180936\n  (7612, 6979)\t0.36843525221185486\n  (7612, 5843)\t0.3163161162704016\n  (7612, 2389)\t0.2972512112854693\n  (7612, 824)\t0.42230849707570395\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## 1.3. Classification with SVM model \n","metadata":{}},{"cell_type":"markdown","source":"* Linear SVM proven good in text classification benchmarks\n\n* Linear SVM’s margin maximization makes it less sensitive to outliers  and noise (which is important in texts with  slang, hashtags, abbreviations and misspellings) compared to other models such as Logistic Regression or Naive Bayes\n\n* and SVM are better at non overfitting sparse high-dimentional data as compared to tree-based models such as Random Forests or Gradient Boosting","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\nfrom sklearn.model_selection import train_test_split, PredefinedSplit, KFold, GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nimport time\nimport psutil, os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:10:48.930268Z","iopub.execute_input":"2025-03-19T15:10:48.930579Z","iopub.status.idle":"2025-03-19T15:10:48.934731Z","shell.execute_reply.started":"2025-03-19T15:10:48.930555Z","shell.execute_reply":"2025-03-19T15:10:48.933733Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"#### Set initial parameters of SVM model which will be further tuned","metadata":{}},{"cell_type":"code","source":"# SVM parameter grid\nsvm_param_grid = {\n    'C': [0.01, 0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['scale', 'auto'],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:07:29.660169Z","iopub.execute_input":"2025-03-19T16:07:29.660494Z","iopub.status.idle":"2025-03-19T16:07:29.664287Z","shell.execute_reply.started":"2025-03-19T16:07:29.660468Z","shell.execute_reply":"2025-03-19T16:07:29.663586Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"#### Set functions to run grid search and eval\n#####  Hyperparams search is done via cross-validation with GridSearchCV with 5 folds\n##### We then run a separate 5‑fold CV on the entire training set with the best hyperparameters to get avarage f1 macro score","metadata":{}},{"cell_type":"code","source":"def get_memory_usage():\n    process = psutil.Process(os.getpid())  #function to get current process memory usage in MB\n    return process.memory_info().rss / (1024 * 1024) \n\n# 1 Hyperparameter Tuning (80–20)\n\ndef tune_svm(X, y, param_grid):\n    \n    cv = KFold(n_splits=5, shuffle=True, random_state=7)\n\n    grid_search = GridSearchCV(\n        estimator=SVC(),\n        param_grid=param_grid,\n        scoring='f1_macro',\n        cv=cv,\n        verbose=1,\n        n_jobs=-1\n    )\n    grid_search.fit(X,y)\n    \n    print(\"\\nBest Score (5-fold CV): {:.3f}\".format(grid_search.best_score_))\n    print(\"Best Params:\", grid_search.best_params_)\n    return grid_search.best_params_\n\n\n\n# 2 5-Fold CV with best hyperparameters\n\ndef evaluate_svm_5fold(X, y, best_params):\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_f1_scores = []\n    fold = 5\n    for train_index, val_index in kf.split(X):\n        print(f\"Training fold {fold}\")\n        X_tr, X_val = X[train_index], X[val_index]\n        y_tr, y_val = y[train_index], y[val_index]\n        \n        model = SVC(**best_params)\n        model.fit(X_tr, y_tr)\n\n        start_inference = time.time()  #start inference timer\n        mem_before = get_memory_usage()\n        \n        preds = model.predict(X_val)\n\n        mem_after = get_memory_usage()    #get memory after inference\n        end_inference = time.time()        #end inference timer\n        inference_time = end_inference - start_inference \n        print(f\"Inference time: {inference_time:.3f} sec, Memory usage diff: {mem_after - mem_before:.3f} MB\")\n\n        \n        fold_f1 = f1_score(y_val, preds, average='macro')\n        print(f\"Fold {fold} f1_macro: {fold_f1:.3f}\")\n        cv_f1_scores.append(fold_f1)\n        fold += 1\n    \n    avg_f1 = np.mean(cv_f1_scores)\n    print(\"Average 5-fold CV f1_macro:\", avg_f1)\n    return avg_f1\n\n\n# 3 Training on full data\n\ndef train_and_predict_svm_full(X, y, X_test, best_params, out_file):\n    final_model = SVC(**best_params)\n    final_model.fit(X, y)\n\n    start_inference = time.time()  #start inference timer for final prediction\n    mem_before = get_memory_usage() #capture memory before final inference\n    \n    test_preds = final_model.predict(X_test)\n\n    mem_after = get_memory_usage()    #capture memory after final inference\n    end_inference = time.time()      #end inference timer for final prediction\n    inference_time = end_inference - start_inference  #final inference time\n    print(f\"Final model inference time: {inference_time:.3f} sec, Memory usage diff: {mem_after - mem_before:.3f} MB\")\n\n    submission_df = pd.DataFrame({'id': test_df['id'], 'target': test_preds})\n    submission_df.to_csv(out_file, index=False)\n    print(f\"\\nSaved {out_file} using the best SVM model\")\n    return final_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:07:35.793941Z","iopub.execute_input":"2025-03-19T16:07:35.794235Z","iopub.status.idle":"2025-03-19T16:07:35.803589Z","shell.execute_reply.started":"2025-03-19T16:07:35.794215Z","shell.execute_reply":"2025-03-19T16:07:35.802743Z"}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"#### Tune SVM hyperparameters on both BoW and TF-IDF train data:","metadata":{}},{"cell_type":"code","source":"y_train = train_df['target']\n\n# Start total timer \nstart_total = time.time()\n\n#  SVM on BoW\n\nprint(\"=== SVM on BoW ===\")\nbow_best_params = tune_svm(X_train_bow, y_train, svm_param_grid)\nprint(\"\\nRunning 5-fold CV (BoW) with best hyperparameters:\")\nevaluate_svm_5fold(X_train_bow, y_train, bow_best_params)\n\n\n#  SVM on TF-IDF\n\nprint(\"\\n=== SVM on TF-IDF ===\")\ntfidf_best_params = tune_svm(X_train_tfidf, y_train, svm_param_grid)\nprint(\"\\nRunning 5-fold CV (TF-IDF) with best hyperparameters:\")\nevaluate_svm_5fold(X_train_tfidf, y_train, tfidf_best_params)\n# train_and_predict_svm_full(X_train_tfidf, y_train, X_test_tfidf, tfidf_best_params, \"SVM_TFIDF_submission.csv\")\n\n\nend_total = time.time()\nprint(\"Total pipeline time (sec):\", end_total - start_total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:08:19.304152Z","iopub.execute_input":"2025-03-19T16:08:19.304491Z","iopub.status.idle":"2025-03-19T16:13:31.504062Z","shell.execute_reply.started":"2025-03-19T16:08:19.304427Z","shell.execute_reply":"2025-03-19T16:13:31.503035Z"}},"outputs":[{"name":"stdout","text":"=== SVM on BoW ===\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\nBest Score (5-fold CV): 0.786\nBest Params: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n\nRunning 5-fold CV (BoW) with best hyperparameters:\nTraining fold 5\nInference time: 0.603 sec, Memory usage diff: 0.000 MB\nFold 5 f1_macro: 0.796\nTraining fold 6\nInference time: 0.607 sec, Memory usage diff: 0.000 MB\nFold 6 f1_macro: 0.785\nTraining fold 7\nInference time: 0.611 sec, Memory usage diff: 0.000 MB\nFold 7 f1_macro: 0.808\nTraining fold 8\nInference time: 0.606 sec, Memory usage diff: -70.250 MB\nFold 8 f1_macro: 0.778\nTraining fold 9\nInference time: 0.588 sec, Memory usage diff: 0.000 MB\nFold 9 f1_macro: 0.781\nAverage 5-fold CV f1_macro: 0.7896273632776335\n\n=== SVM on TF-IDF ===\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\nBest Score (5-fold CV): 0.780\nBest Params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n\nRunning 5-fold CV (TF-IDF) with best hyperparameters:\nTraining fold 5\nInference time: 0.674 sec, Memory usage diff: 0.000 MB\nFold 5 f1_macro: 0.777\nTraining fold 6\nInference time: 0.690 sec, Memory usage diff: 0.000 MB\nFold 6 f1_macro: 0.783\nTraining fold 7\nInference time: 0.680 sec, Memory usage diff: 0.000 MB\nFold 7 f1_macro: 0.806\nTraining fold 8\nInference time: 0.667 sec, Memory usage diff: 0.000 MB\nFold 8 f1_macro: 0.783\nTraining fold 9\nInference time: 0.665 sec, Memory usage diff: 0.000 MB\nFold 9 f1_macro: 0.774\nAverage 5-fold CV f1_macro: 0.784493801943656\nTotal pipeline time (sec): 312.19361305236816\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"### Quality measured with avarage f1 macro across show that\n#### * Both BoW and TF-IDF pipelines achieve practically similar average F1_macro scores ~0.79,\n#### * with SVM on BoW having a slightly higher score of 0.789\n\n### Inference Time:\n#### For BoW, each fold’s inference takes roughly 0.45–0.47 seconds.\n#### For TF-IDF, the inference time is slightly higher (~0.5 seconds per fold)\n\n#### The overall pipeline runtime uquals 271 seconds (it includes hyperparameter tuning, cross-validation, and final model training)","metadata":{}},{"cell_type":"markdown","source":"## Best model -- SVM on BoW\n#### Make predictions for test data using SVM on BoW:","metadata":{}},{"cell_type":"code","source":"train_and_predict_svm_full(X_train_bow, y_train, X_test_bow, bow_best_params, \"1_SVM_BoW_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:17:41.266626Z","iopub.execute_input":"2025-03-19T18:17:41.266985Z","iopub.status.idle":"2025-03-19T18:17:47.332037Z","shell.execute_reply.started":"2025-03-19T18:17:41.266958Z","shell.execute_reply":"2025-03-19T18:17:47.331243Z"}},"outputs":[{"name":"stdout","text":"Final model inference time: 1.355 sec, Memory usage diff: 0.000 MB\n\nSaved 1_SVM_BoW_submission.csv using the best SVM model\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"SVC(C=0.1, kernel='linear')","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":60},{"cell_type":"markdown","source":"# 2. NN based model -- 1D CNN","metadata":{}},{"cell_type":"markdown","source":"### For the classification we choose a simple 1D CNN\nTweets are overall of short length and in them  n-grams or key phrases carry strong signals , CNNs are good at at identifying *local n-gram features for short-text classification*\n#### Convolutions are best at detecting patterns over short contexts — exactly what we have in tweets\n#### Plus they have fewer parameters and are trained faster than  than sequential models like LSTM or RNNs\n","metadata":{}},{"cell_type":"markdown","source":"####  the architecture — Embedding layer, Conv1D + GlobalMaxPooling1D + fully connected dense output\n* The architecture starts with an embedding layer to convert words into dense vectors, \n* followed by a convolutional layer that extracts meaningful features (with tunable filter sizes and kernel sizes to experiment with different local contexts),\n* then a global max pooling layer that condenses these features, \n* and finally a dense layer with dropout for regularization before the binary output.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"#### WE set up parameters for CNN \n","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport keras_tuner as kt\n\n\ndef get_memory_usage():\n    process = psutil.Process(os.getpid()) \n    return process.memory_info().rss / (1024 * 1024) \n\n\n# --- PARAMS ---\nmax_words = 10000  # we'll use top 10k words in vocab\nmaxlen = 100       # max tweet length in words\n\n\n# Tokenize and pad training texts\n# Initialize Keras tokenizer and fit it  on the processed tweet texts for vocab learning\n# train_df tweet entrees get converted to int sequence with resulting sequences padded\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_df['processed_text'])\nX_train_seq = tokenizer.texts_to_sequences(train_df['processed_text'])\nX_train = pad_sequences(X_train_seq, maxlen=maxlen)\ny_train = train_df['target'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:04:11.661527Z","iopub.execute_input":"2025-03-19T18:04:11.661862Z","iopub.status.idle":"2025-03-19T18:04:12.212563Z","shell.execute_reply.started":"2025-03-19T18:04:11.661841Z","shell.execute_reply":"2025-03-19T18:04:12.211621Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"#### 2.1 Trainng the model\nThe build_model function defines a CNN with \n* **Embedding layer,**           ---> Pretrained embeddings capture semantic similarity among words and thus boost model performance\n* **one Conv1D layer** (with tunable filters and kernel size),  ----> CNN setup for text classification often includes parallel convolutional layers with different filter widths, followed by a global pooling layer. Kernel sizes help capture patterns of varying n-gram lengths.\n* **GlobalMaxPooling1D layer,** ---> help capture the most salient features across the entire sentence\n* **Dense layer, dropout,**\n* **Sigmoid output**\n\n\n**The hyperparameter ranges are chosen as follows** --- consideration for choice: standard hyperparams values and ranges for text classification:\n* **embedding dimensions of 50, 100, 200**\n* **filter sizes: 64, 128, 256**\n* **kernel sizes 3, 5,  7**\n* **dense units: 32, 64, 128**\n* **dropout rates rannge from 0.2 to 0.5 with stepsize=0.1**\n#### The values gove a balance between model complexity and overfitting: the embedding dimension is high enough to capture semantic nuances, \n#### while the convolutional filters and kernel sizes let us experiment with various n-gram sizes without overly complicating the network\n#### The dropout controls model capacity and regularize against overfitting","metadata":{}},{"cell_type":"code","source":"# Model buiding for hyperparameter tuning\n\n\ndef build_model(hp):\n    \"\"\"\n    function to build 1D CNN model:\n    \n    The model architecture:\n      - Embedding layer with tunable embedding dimension\n      - Conv1D layer with tunable number of filters and kernel size\n      - GlobalMaxPooling1D layer to reduce sequence dimensions\n      - Dense layer with tunable units.\n      - Dropout layer with tunable dropout rate to reduce overfitting\n      - Final Dense output layer with sigmoid activation for binary classification\n      \n    Hyperparameters tuned via Keras Tuner include:\n      - 'embedding_dim': dimension of the word embeddings\n      - 'filters': number of  convolution filters (ie output channels) in the Conv1D layer\n      - 'kernel_size': width of the convolutional kernel\n      - 'dense_units': number of units in the Dense layer\n      - 'dropout_rate': dropout rate after the Dense layer/ proportion of neuron outputs dropped to reduce overfitting\n    \n    Args:\n        hp: HyperParameters object for tuning\n        \n    Returns:\n        A compiled Keras model\n    \"\"\"\n    model = tf.keras.Sequential()\n    \n    # tunable embedding dimension: we will try 50, 100 , 200\n    # Keras layer to lookup table that maps each of the top max_words tokens (from the Tokenizer) to a vector of size embedding_dim\n    embedding_dim = hp.Choice('embedding_dim', values=[50, 100, 200])\n    model.add(tf.keras.layers.Embedding(input_dim=max_words,\n                                        output_dim=embedding_dim,\n                                        input_length=maxlen))\n    \n    # Tunable Conv1D layer with tunable filters and kernel size\n    filters = hp.Choice('filters', values=[64, 128, 256])\n    kernel_size = hp.Choice('kernel_size', values=[3, 5, 7])\n    model.add(tf.keras.layers.Conv1D(filters=filters,\n                                     kernel_size=kernel_size,\n                                     activation='relu'))\n    \n    # Global max pooling layer to capture the most salient features\n    model.add(tf.keras.layers.GlobalMaxPooling1D())\n    \n    # dense layer with tunable number of units\n    dense_units = hp.Choice('dense_units', values=[32, 64, 128])\n    model.add(tf.keras.layers.Dense(dense_units, activation='relu'))\n    \n    # Dropout layer with tunable dropout rate\n    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n    model.add(tf.keras.layers.Dropout(dropout_rate))\n    \n    # output layer for binary classification\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:04:19.277215Z","iopub.execute_input":"2025-03-19T18:04:19.277570Z","iopub.status.idle":"2025-03-19T18:04:19.285900Z","shell.execute_reply.started":"2025-03-19T18:04:19.277542Z","shell.execute_reply":"2025-03-19T18:04:19.285000Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"## Hyperparameters tuning with keras tuner \nWe will use Keras Tuner’s Hyperband to search over \n\nspecified choices for embedding dimension, number of filters, kernel size, dense units and dropout rate.\n\nThe goal is to max validation accuracy over a 5-fold-like setup with early stopping to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"# Start total timer \nstart_total = time.time()\n\n\n# Hyperparameter tuning with Keras Tuner (80-20 split)\nstart_tuning = time.time()\ntuner = kt.Hyperband(\n    build_model,\n    objective='val_accuracy', \n    max_epochs=10,\n    factor=3,\n    directory='hyper_tuning',\n    project_name='cnn_disaster'\n)\n\n# early stopping callback to prevent overfitting\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\n# 80-20 train/validation split for hyperparams search\ntuner.search(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])\n\n# best hyperparameters and best model\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Optimal embedding_dim:\", best_hps.get('embedding_dim'))\nprint(\"Optimal filters:\", best_hps.get('filters'))\nprint(\"Optimal kernel_size:\", best_hps.get('kernel_size'))\nprint(\"Optimal dense_units:\", best_hps.get('dense_units'))\nprint(\"Optimal dropout_rate:\", best_hps.get('dropout_rate'))\nend_tuning = time.time()\nprint(\"Hyperparameter tuning time (sec):\", end_tuning - start_tuning)\n\n\n# 5-Fold Cross-Validation using the best hyperparameters\nstart_cv = time.time()\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = []\ncv_times = []\nfold = 1\n\nfor train_index, val_index in kf.split(X_train):\n    print(f\"Training fold {fold}\")\n    X_tr, X_val = X_train[train_index], X_train[val_index]\n    y_tr, y_val = y_train[train_index], y_train[val_index]\n    model = build_model(best_hps)\n    start_time = time.time()\n    history = model.fit(X_tr, y_tr, epochs=10, validation_data=(X_val, y_val), callbacks=[stop_early], verbose=1)\n    fold_time = time.time() - start_time\n    cv_times.append(fold_time)\n    print(f\"Fold {fold} training time: {fold_time:.2f} seconds\")\n\n    start_inference = time.time()  #start inference timer for CV inference\n    mem_before = get_memory_usage()  #capture memory before inference\n    \n    predictions = model.predict(X_val)  \n    \n    mem_after = get_memory_usage()  #capture memory after inference\n    end_inference = time.time()  #end inference timer for CV inference\n    inference_time = end_inference - start_inference  \n    print(f\"Inference time: {inference_time:.3f} sec, Memory usage diff: {mem_after - mem_before:.3f} MB\")\n\n    \n    preds = (predictions > 0.5).astype(int).reshape(-1)\n    fold_f1 = f1_score(y_val, preds, average='macro')\n    # score = model.evaluate(X_val, y_val, verbose=0)\n    print(f\"Fold {fold} f1_macro: {fold_f1}\")\n    cv_scores.append(fold_f1)\n    fold += 1\nend_cv = time.time()\nprint(\"Average CV f1_macro:\", np.mean(cv_scores))\nprint(\"Average CV training time (sec):\", np.mean(cv_times))\nprint(\"Total CV time (sec):\", end_cv - start_cv)\n\nend_total = time.time()\nprint(\"Total pipeline time (sec):\", end_total - start_total)\n\n# Retrain best model on full training set using best hyperparameters\nstart_final = time.time()\nbest_model = build_model(best_hps)\nhistory = best_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early], verbose=1)\nend_final = time.time()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:04:31.162372Z","iopub.execute_input":"2025-03-19T18:04:31.162688Z","iopub.status.idle":"2025-03-19T18:08:38.722940Z","shell.execute_reply.started":"2025-03-19T18:04:31.162666Z","shell.execute_reply":"2025-03-19T18:08:38.722015Z"}},"outputs":[{"name":"stdout","text":"Trial 30 Complete [00h 00m 07s]\nval_accuracy: 0.7583716511726379\n\nBest val_accuracy So Far: 0.7813525795936584\nTotal elapsed time: 00h 03m 30s\nOptimal embedding_dim: 100\nOptimal filters: 256\nOptimal kernel_size: 7\nOptimal dense_units: 128\nOptimal dropout_rate: 0.4\nHyperparameter tuning time (sec): 209.85265111923218\nTraining fold 1\nEpoch 1/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6447 - loss: 0.6177 - val_accuracy: 0.7984 - val_loss: 0.4450\nEpoch 2/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8508 - loss: 0.3573 - val_accuracy: 0.7833 - val_loss: 0.5133\nEpoch 3/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9334 - loss: 0.1913 - val_accuracy: 0.7708 - val_loss: 0.6629\nEpoch 4/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1028 - val_accuracy: 0.7525 - val_loss: 0.8164\nFold 1 training time: 6.27 seconds\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\nInference time: 0.601 sec, Memory usage diff: 2.000 MB\nFold 1 f1_macro: 0.7504725051748498\nTraining fold 2\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6605 - loss: 0.6088 - val_accuracy: 0.7905 - val_loss: 0.4512\nEpoch 2/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8572 - loss: 0.3477 - val_accuracy: 0.7827 - val_loss: 0.5225\nEpoch 3/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9290 - loss: 0.1947 - val_accuracy: 0.7577 - val_loss: 0.6796\nFold 2 training time: 5.55 seconds\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nInference time: 0.438 sec, Memory usage diff: 0.250 MB\nFold 2 f1_macro: 0.7545849354460479\nTraining fold 3\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6482 - loss: 0.6148 - val_accuracy: 0.8181 - val_loss: 0.4380\nEpoch 2/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8420 - loss: 0.3676 - val_accuracy: 0.8050 - val_loss: 0.4731\nEpoch 3/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9231 - loss: 0.2082 - val_accuracy: 0.7892 - val_loss: 0.6101\nEpoch 4/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9620 - loss: 0.1079 - val_accuracy: 0.7840 - val_loss: 0.7412\nFold 3 training time: 6.15 seconds\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nInference time: 0.432 sec, Memory usage diff: 2.375 MB\nFold 3 f1_macro: 0.7792894599481555\nTraining fold 4\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6492 - loss: 0.6131 - val_accuracy: 0.8016 - val_loss: 0.4548\nEpoch 2/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8469 - loss: 0.3642 - val_accuracy: 0.7753 - val_loss: 0.5430\nEpoch 3/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9287 - loss: 0.2036 - val_accuracy: 0.7714 - val_loss: 0.7070\nFold 4 training time: 6.07 seconds\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\nInference time: 0.501 sec, Memory usage diff: 0.750 MB\nFold 4 f1_macro: 0.7576433121019108\nTraining fold 5\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6612 - loss: 0.5973 - val_accuracy: 0.7891 - val_loss: 0.4539\nEpoch 2/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8615 - loss: 0.3363 - val_accuracy: 0.7852 - val_loss: 0.5343\nEpoch 3/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9372 - loss: 0.1840 - val_accuracy: 0.7668 - val_loss: 0.6514\nFold 5 training time: 5.58 seconds\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nInference time: 0.429 sec, Memory usage diff: 1.875 MB\nFold 5 f1_macro: 0.761730572228521\nAverage CV f1_macro: 0.7607441569798971\nAverage CV training time (sec): 5.923155164718628\nTotal CV time (sec): 32.09015941619873\nTotal pipeline time (sec): 241.94412469863892\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6464 - loss: 0.6121 - val_accuracy: 0.7676 - val_loss: 0.4707\nEpoch 2/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8380 - loss: 0.3682 - val_accuracy: 0.7695 - val_loss: 0.5653\nEpoch 3/10\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9272 - loss: 0.2112 - val_accuracy: 0.7091 - val_loss: 0.8935\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"##### Hyperparameter tuning time: ~210 seconds\n##### Average CV F1 macro (5 folds): ~0.761\n##### Training and inference Time: \n##### Training time per fold: ~5.92 seconds on average, leading to a total CV time of ~32.09 seconds\n##### After each fold’s training, the inference takes 0.48 secs on average \n##### Memory usage differences are in the range of 0.25 - 2.38  MB, which means additional memory used during inference.\n\n\n#### The entire pipeline took ~242 seconds","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Predictions on test data","metadata":{}},{"cell_type":"code","source":"# Preprocess test data, make predictions\nX_test_seq = tokenizer.texts_to_sequences(test_df['processed_text'])\nX_test = pad_sequences(X_test_seq, maxlen=maxlen)\n\nstart_inference = time.time()  #start inference timer for test predictions\nmem_before = get_memory_usage()  #capture memory before test inference\n\ntest_preds = best_model.predict(X_test)\n\nmem_after = get_memory_usage()  #capture memory after test inference\nend_inference = time.time()  #end inference timer for test predictions\ninference_time = end_inference - start_inference  \nprint(f\"Test inference time: {inference_time:.3f} sec, Memory usage diff: {mem_after - mem_before:.3f} MB\") \n\ntest_preds = (test_preds > 0.5).astype(int).reshape(-1)\n\nsubmission = pd.DataFrame({'id': test_df['id'], 'target': test_preds})\nsubmission.to_csv(\"2_submission_cnn_tuned.csv\", index=False)\nprint(\"\\nSaved submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:18:19.490239Z","iopub.execute_input":"2025-03-19T18:18:19.490605Z","iopub.status.idle":"2025-03-19T18:18:19.761746Z","shell.execute_reply.started":"2025-03-19T18:18:19.490578Z","shell.execute_reply":"2025-03-19T18:18:19.760963Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\nTest inference time: 0.216 sec, Memory usage diff: 1.250 MB\n\nSaved submission.csv\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"# 3. Fine-tuning pre-trained model -- Roberta-base ","metadata":{}},{"cell_type":"markdown","source":"For fine-tuning we choose Roberta-base model\n\nwe are going to:\n* tokenize using RobertaTokenizerFast\n* use RobertaForSequenceClassification (pretrained roberta-base)\n* fine-tune using Trainer with chosen hyperparameters\n* predict on test data ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification\nfrom transformers import Trainer, TrainingArguments, TrainerCallback\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nimport time\nimport psutil\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:41:18.384893Z","iopub.execute_input":"2025-03-19T18:41:18.385214Z","iopub.status.idle":"2025-03-19T18:41:18.388996Z","shell.execute_reply.started":"2025-03-19T18:41:18.385186Z","shell.execute_reply":"2025-03-19T18:41:18.388261Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def get_memory_usage():\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / (1024 * 1024)\n\n\ntrain_texts = train_df['processed_text'].tolist()\ntrain_labels = train_df['target'].values\ntest_texts = test_df['processed_text'].tolist()\ntest_ids = test_df['id'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:41:23.310044Z","iopub.execute_input":"2025-03-19T18:41:23.310337Z","iopub.status.idle":"2025-03-19T18:41:23.318233Z","shell.execute_reply.started":"2025-03-19T18:41:23.310314Z","shell.execute_reply":"2025-03-19T18:41:23.317493Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"### train/validation splitting:\n#### We do 80-20 split for small validation set","metadata":{}},{"cell_type":"code","source":"# Split into train/val\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_texts, train_labels, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:41:32.149278Z","iopub.execute_input":"2025-03-19T18:41:32.149637Z","iopub.status.idle":"2025-03-19T18:41:32.156220Z","shell.execute_reply.started":"2025-03-19T18:41:32.149606Z","shell.execute_reply":"2025-03-19T18:41:32.155547Z"}},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"#### we use RobertaTokenizerFast as tokenizer:\nto split text into subword tokens consistent with Roberta-base vocabulary","metadata":{}},{"cell_type":"code","source":"# Tokenize (max_length=128)\ntokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\nval_encodings   = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\ntest_encodings  = tokenizer(test_texts, truncation=True, padding=True, max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:41:40.989653Z","iopub.execute_input":"2025-03-19T18:41:40.989942Z","iopub.status.idle":"2025-03-19T18:41:41.653368Z","shell.execute_reply.started":"2025-03-19T18:41:40.989920Z","shell.execute_reply":"2025-03-19T18:41:41.652719Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"#### We create dataset objects\nto wrap tokenized inputs for further Trainer input","metadata":{}},{"cell_type":"code","source":"#  Create Dataset class\nclass TweetDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Pytorch dataset to hold tokenized texts and labels \n    \"\"\"\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ntrain_dataset = TweetDataset(train_encodings, train_labels)\nval_dataset   = TweetDataset(val_encodings,   val_labels)\ntest_dataset  = TweetDataset(test_encodings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:41:49.577650Z","iopub.execute_input":"2025-03-19T18:41:49.577940Z","iopub.status.idle":"2025-03-19T18:41:49.610368Z","shell.execute_reply.started":"2025-03-19T18:41:49.577920Z","shell.execute_reply":"2025-03-19T18:41:49.609556Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"#### We use RobertaForSequenceClassification\n##### with num_labels=2 for binary classification","metadata":{}},{"cell_type":"code","source":"# Callback for logging\nclass LoggingCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            print(f\"Step {state.global_step}: {logs}\")\n\n# Compute metrics\nfrom sklearn.metrics import f1_score, accuracy_score\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': accuracy_score(labels, predictions),\n        'f1_macro': f1_score(labels, predictions, average='macro')\n    }\n\n# Initialize model\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n\n# Training config\nbatch_size = 32\nepochs = 3\nlearning_rate = 2e-5\n\n# Training configuration with Hugging Face Trainer \ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    learning_rate=learning_rate,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=[\"none\"]\n)\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[LoggingCallback()]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:42:04.139892Z","iopub.execute_input":"2025-03-19T18:42:04.140174Z","iopub.status.idle":"2025-03-19T18:42:04.703125Z","shell.execute_reply.started":"2025-03-19T18:42:04.140154Z","shell.execute_reply":"2025-03-19T18:42:04.702413Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"#### The model is fine-tuned and evaluated on the validation set:","metadata":{}},{"cell_type":"code","source":"# Train\nstart_total = time.time()\n# fine-tune the model\ntrainer.train()\n\n\n# Evaluate\nstart_inference = time.time()\nmem_before = get_memory_usage()\nval_metrics = trainer.evaluate()\nmem_after = get_memory_usage()\nend_inference = time.time()\nprint(\"Evaluation Inference time: {:.3f}s, Memory usage diff: {:.3f} MB\"\n      .format(end_inference - start_inference, mem_after - mem_before))\nprint(\"Validation metrics:\", val_metrics)\n\nend_total = time.time()\nprint(\"Total time (sec):\", end_total - start_total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:43:26.407797Z","iopub.execute_input":"2025-03-19T18:43:26.408134Z","iopub.status.idle":"2025-03-19T18:45:17.643742Z","shell.execute_reply.started":"2025-03-19T18:43:26.408108Z","shell.execute_reply":"2025-03-19T18:45:17.642603Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='573' max='573' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [573/573 01:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Model Preparation Time</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.492400</td>\n      <td>0.435452</td>\n      <td>0.003200</td>\n      <td>0.812869</td>\n      <td>0.808881</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.420000</td>\n      <td>0.420721</td>\n      <td>0.003200</td>\n      <td>0.825345</td>\n      <td>0.816686</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.364700</td>\n      <td>0.440592</td>\n      <td>0.003200</td>\n      <td>0.820092</td>\n      <td>0.814986</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Step 50: {'loss': 0.6239, 'grad_norm': 7.579275608062744, 'learning_rate': 1.825479930191972e-05, 'epoch': 0.2617801047120419}\nStep 100: {'loss': 0.5207, 'grad_norm': 5.010350704193115, 'learning_rate': 1.6509598603839444e-05, 'epoch': 0.5235602094240838}\nStep 150: {'loss': 0.4924, 'grad_norm': 6.428791046142578, 'learning_rate': 1.4764397905759162e-05, 'epoch': 0.7853403141361257}\nStep 191: {'eval_loss': 0.43545156717300415, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.8128693368351937, 'eval_f1_macro': 0.8088813977541567, 'eval_runtime': 2.3577, 'eval_samples_per_second': 645.961, 'eval_steps_per_second': 20.359, 'epoch': 1.0}\nStep 200: {'loss': 0.4464, 'grad_norm': 7.938277244567871, 'learning_rate': 1.3019197207678885e-05, 'epoch': 1.0471204188481675}\nStep 250: {'loss': 0.4091, 'grad_norm': 7.515224456787109, 'learning_rate': 1.1273996509598604e-05, 'epoch': 1.3089005235602094}\nStep 300: {'loss': 0.4095, 'grad_norm': 6.289299488067627, 'learning_rate': 9.528795811518325e-06, 'epoch': 1.5706806282722514}\nStep 350: {'loss': 0.42, 'grad_norm': 6.506678581237793, 'learning_rate': 7.783595113438046e-06, 'epoch': 1.8324607329842932}\nStep 382: {'eval_loss': 0.42072129249572754, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.8253447143795141, 'eval_f1_macro': 0.8166860334629279, 'eval_runtime': 2.3136, 'eval_samples_per_second': 658.286, 'eval_steps_per_second': 20.747, 'epoch': 2.0}\nStep 400: {'loss': 0.3737, 'grad_norm': 7.164631366729736, 'learning_rate': 6.038394415357767e-06, 'epoch': 2.094240837696335}\nStep 450: {'loss': 0.3396, 'grad_norm': 8.85330867767334, 'learning_rate': 4.293193717277487e-06, 'epoch': 2.356020942408377}\nStep 500: {'loss': 0.3406, 'grad_norm': 7.081439018249512, 'learning_rate': 2.547993019197208e-06, 'epoch': 2.6178010471204187}\nStep 550: {'loss': 0.3647, 'grad_norm': 9.044632911682129, 'learning_rate': 8.027923211169285e-07, 'epoch': 2.8795811518324608}\nStep 573: {'eval_loss': 0.44059208035469055, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.8200919238345371, 'eval_f1_macro': 0.8149863446123289, 'eval_runtime': 2.3074, 'eval_samples_per_second': 660.052, 'eval_steps_per_second': 20.803, 'epoch': 3.0}\nStep 573: {'train_runtime': 108.1544, 'train_samples_per_second': 168.925, 'train_steps_per_second': 5.298, 'total_flos': 488214896551200.0, 'train_loss': 0.4273335388609668, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Step 573: {'eval_loss': 0.44059208035469055, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.8200919238345371, 'eval_f1_macro': 0.8149863446123289, 'eval_runtime': 2.2994, 'eval_samples_per_second': 662.333, 'eval_steps_per_second': 20.875, 'epoch': 3.0}\nEvaluation Inference time: 2.305s, Memory usage diff: 0.000 MB\nValidation metrics: {'eval_loss': 0.44059208035469055, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.8200919238345371, 'eval_f1_macro': 0.8149863446123289, 'eval_runtime': 2.2994, 'eval_samples_per_second': 662.333, 'eval_steps_per_second': 20.875, 'epoch': 3.0}\nTotal time (sec): 111.2301332950592\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"* training (for all 3 epochs) took ~108.15 secs\n* inference Time (final evaluation) ~2.30 secs\n* Overall Pipeline took ~111.23 secs\n  \n*  Avarage Validation Accuracy = 0.82  and  avarage F1 Macro = 0.81","metadata":{}},{"cell_type":"markdown","source":"#### Make predictons on test data:","metadata":{}},{"cell_type":"code","source":"# inference time and memory usage for test prediction\nstart_inference = time.time()\nmem_before = get_memory_usage()\n\npreds_output = trainer.predict(test_dataset)\n\nmem_after = get_memory_usage()\nend_inference = time.time()\nprint(\"Prediction Inference time: {:.3f}s, Memory usage diff: {:.3f} MB\"\n      .format(end_inference - start_inference, mem_after - mem_before))\n\nlogits = preds_output.predictions\npreds = np.argmax(logits, axis=1)\nsubmission_df = pd.DataFrame({\"id\": test_ids, \"target\": preds})\nsubmission_df.to_csv(\"3_roberta_submission.csv\", index=False)\nprint(\"Submission saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:47:54.864845Z","iopub.execute_input":"2025-03-19T18:47:54.865178Z","iopub.status.idle":"2025-03-19T18:47:59.928400Z","shell.execute_reply.started":"2025-03-19T18:47:54.865151Z","shell.execute_reply":"2025-03-19T18:47:59.927768Z"}},"outputs":[{"name":"stdout","text":"Prediction Inference time: 5.054s, Memory usage diff: 0.000 MB\nSubmission saved\n","output_type":"stream"}],"execution_count":73},{"cell_type":"markdown","source":"ALL 3 models were assessed with aravage f1 macro score and resousres intake were estimated with train/fine-tuning + inference time + memory usage for train+inference\n\nThe model with highest quality/resources (i.e. av.f1 macro/memory+time) ration is Roberta-base pre-trained model fine-tuned on the tweet dataset ","metadata":{}},{"cell_type":"markdown","source":"3 models were assessed:\n* SVM on BoW\n* 1D CNN model\n* pre-trained ROBERTa-base transformer model\n  \nAll 3 models were compared by examining their macro-F1 scores (averaged over the validation folds/test set) and by estimating their overall resource usage (training/fine-tuning plus inference time and memory consumption). \n\n* SVM on BoW = 0.78/312\n* 1D CNN model = 0.76/242 \n* pre-trained ROBERTa-base transformer model = 0.81/ 111  \n\nBased on this performance-to-resource ratio, the pretrained RoBERTa-base model fine-tuned on the tweet dataset yielded the highest score relative to SVM and CNN.\n\nFor all of 3 models avaraged f1 macro lies in very close range to 0.80\nThe total pipelines time though differs, but not that greatly either (all <=5 min)","metadata":{}},{"cell_type":"markdown","source":"Encountered dfficulties:\n* 1) The main challenge consists in proper data preprocessing -- in preprocessing step how to manage (remove/change) elements assosiated with/found often in tweets:\n  *  like hashtaged compound elements  like #loveyouall for which **wordninja** was used to split them, anothough it still doesnt guarantee all splitting would result in correct tokens (like \"love you all\" or \"love youal\"? ). hashtags serve as keywords in tweets but dividing them in correct tokens is challenging.\n  *  or informal abbreviations, slang which may not stem correctly using SnowballStemmer, potentially losing critical sentiment or context.\n  *  or  finding the correct words or n-grams to keep out(which should not be removed and sometimes change tweet meaning of tweet from dorect one to figurative one)\n  * anpther question is how to combine other 2 cols from train_df (info on location and keywords) with tweet data and whether they carry any particular role\n* 2) Hyperparameters search for CNN:CNN with too many filters or large kernel sizes might overfit to specific patterns , balancing between capturing local n-gram features and generalizing across the dataset is challenging\n  3) Sensitivity to hyperparameter for fine-tuning of ROBERTa : we fine-tune pre-trained transformer with a small learning rate (2e-5) and 3 epochs but its performance is changes strongly with even small change in these hyperparams, for ex increasing number of epochs >5 leads to very strong overfitting signifincantly reducing performance score.\n  ","metadata":{}},{"cell_type":"markdown","source":"How to potentially improve the results:\n1) Better preprocessing : we can replace stemming with contextual embeddings (for ex BERT, RoBERTa) and subword tokenization ( for ex BPE or WordPiece) to handle out-of-vocabulary abbreviations.\n2) for svm: we could perform dimentionality reductinon (for ex using SVD or LDA) to reduce feature space complexity.\n3) for cnn we can try 1) increasnig convolutional layers and very kernel sizes to capture different n-gram patterns 2) integrating pretrained embeddings like GloVe or FastText to improve feature representation and reduce training time\n5) for both cnn and roberta we could apply learning rate scheduling methods to adjust learnign rate dynalically during training","metadata":{}},{"cell_type":"markdown","source":"source of code: code from lectures (for preprocessing + BoW, TD-IDF vectorization)\n                o1 model for roberta-base implementation ","metadata":{}}]}